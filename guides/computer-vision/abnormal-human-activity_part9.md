---  
last_modified_on: "2023-05-27"
title: Abnormal Human Activity Recognition (Part 9 - Discussion)
description: A review about Abnormal Human Activity Recognition (Deep features based) (cont.).
series_position: 36
author_github: https://github.com/aioz-ai
tags: ["type: insight", "level: easy", "guides: abnormal_activity_recognition"]
---  
# Discussion

For vision-based human action recognition systems that classify human actions as normal or abnormal in a variety of applications such as AAL, smart homes, crowd analysis, and so on, a compact representation of the action videos must be defined while taking into account the discrimination ability of the description as well as time and computational complexity. The blog series discussed three techniques for expressing actions for abnormal human identification applications: 2D AbHAR, 3D AbHAR, and deep recognition systems for abnormal activities.

During the survey, it was discovered that each method had its drawbacks. As a result, the method of action representation must always be application specific. For RGB images/video sequences, abnormal action detection and recognition may be divided into two approaches: single person based and multiple person based. It has been observed that silhouette and spatiotemporal based techniques have been fairly popular for single person based AbHAR. [Khan and Sohn (2011)](https://ieeexplore.ieee.org/abstract/document/6131162) use R Transform on silhouettes and KDA to increase the descriptor's discriminating capacity. However, the system is tested using a hypothetical dataset. Furthermore, [(Sacco et al., 2012)](https://www.tandfonline.com/doi/full/10.2147/CIA.S36297) has a significant implementation cost and is incapable of doing long-term real-time analysis. In contrast, [Riboni et al. (2016)](https://ieeexplore.ieee.org/document/7457139) use a real-time dataset. However, the LOTAR framework was designed for early identification of abnormal human behavior in real time. This technique, however, is dependent on several sensors other than the vision sensor-camera. The integration of sensors, as well as the simultaneous collection and analysis of data from several sensors, has become another real-time challenge. For abnormal crowd behavior detection with multiple people, spatiotemporal features ([Roshtkhari and Levine, 2013](https://www.sciencedirect.com/science/article/abs/pii/S1077314213001239); [Roshtkhari and Levine, 2013](https://openaccess.thecvf.com/content_cvpr_2013/papers/Roshtkhari_Online_Dominant_and_2013_CVPR_paper.pdf)) offer important aspects. Sparseness [(Chathuramali et al., 2014)](https://ieeexplore.ieee.org/document/7069592) of derived features is also important in detecting abnormal crowd behavior such as stampede.

Skeleton and depth creation of a human is achievable for indoor activities where depth cameras may be installed. During the survey, it was observed that in a closed environment, single person-based abnormal activities are studied with smart homes, geriatric health care, and fall detection applications in mind. Full Procrustes distance [(Rougier et al., 2011b)](https://ieeexplore.ieee.org/document/5733403), OCSVM [(Yu et al., 2013)](https://ieeexplore.ieee.org/abstract/document/6566012), History Triple Factor (HTF) [(Goudelis et al., 2015)](https://dl.acm.org/doi/10.1145/2769493.2769562), and BoCSS [(Ma et al., 2014)](https://ieeexplore.ieee.org/document/6730899) approaches detect the fall quickly while dealing with occlusion and view variations well. However, ([Ma et al., 2014](https://ieeexplore.ieee.org/document/6730899); [Goudelis et al., 2015](https://dl.acm.org/doi/10.1145/2769493.2769562)) have a high computing cost and do not provide real-time fall detection, while [(Yu et al., 2013)](https://ieeexplore.ieee.org/abstract/document/6566012) are only tested for one type of fall dataset.

To understand long-term details, the developed single person deep abnormal behavior recognition methods ([Hammerla et al., 2016](https://dl.acm.org/doi/10.5555/3060832.3060835); [Arifoglu and Bouchachia, 2017](https://www.sciencedirect.com/science/article/pii/S1877050917313005); [Park et al., 2018](https://ieeexplore.ieee.org/document/8355147)) used variants of RNN-Vanilla RNNs (VRNN), Long Short-Term RNNs (LSTM), Gated Recurrent Unit RNNs (GRU), and Residual-RNN architectures. However, due to the scarcity of sufficiently large abnormal datasets for deep networks powered by millions of data points, no vision-based single person deep abnormal behavior identification model has yet to be developed. There are plenty of datasets for AbHAR with multiple people, which is why many experiments have combined CNNs and LSTMs ([Vignesh et al., 2017](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w34/papers/Vignesh_Abnormal_Event_Detection_CVPR_2017_paper.pdf); [Medel and Savakis, 2016](https://arxiv.org/abs/1612.00390); [Li and Chuah, 2018](https://ieeexplore.ieee.org/document/8354150?denied=); [Hinami et al., 2017](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hinami_Joint_Detection_and_ICCV_2017_paper.pdf); [Ravanbakhsh et al., 2016](https://ieeexplore.ieee.org/document/8354292)), or used Generative Adversarial Nets [(Ravanbakhsh et al., 2017)](https://ieeexplore.ieee.org/document/8296547/) to create the deep model. However, some of these frameworks rely on data that is not properly labeled, and some are not able to achieve both fast and accurate performance in real time.

---
last_modified_on: "2023-02-19"
title: "AIOZ AI Research in a nutshell (Q1 2023)"
description: 'ðŸŽ‰ AIOZ Publications @ Top-tier Conferences & Journal in Q1-Q2, 2023.'
author_github: "https://github.com/aioz-ai"
hide_on_release_notes: false
pr_numbers: []
year: "2023 "
tags: ["type: highlights","domain: research"]
---
import Alert from '@site/src/components/Alert';

<Alert type="info">
<ul className="text--bold">
We are thrilled to present our latest research achievements during the first quarter of 2023 with the wider community. Our team has been actively participating in top-tier AI conferences and have made significant contributions to the field. We are proud to showcase our latest publications in these flagship conferences, where our cutting-edge research has been recognized and appreciated by the industry. We look forward to sharing our findings and engaging in discussions with other researchers in the field. So join us on this exciting journey and let's explore the forefront of AI research together. Cheers!<br/><br/>
</ul>
</Alert>

## AIOZ AI @ WACV 2023
### âœ¨ Uncertainty-aware Label Distribution Learning for Facial Expression Recognition
:rocket: Github: https://github.com/aioz-ai/

[[Paper](https://openaccess.thecvf.com/content/WACV2023/papers/Le_Uncertainty-Aware_Label_Distribution_Learning_for_Facial_Expression_Recognition_WACV_2023_paper.pdf)] 

*Despite significant progress over the past few years, ambiguity is still a key challenge in Facial Expression Recognition (FER). It can lead to noisy and inconsistent annotation, which hinders the performance of deep learning models in real-world scenarios. In this paper, we propose a new uncertainty-aware label distribution learning method to improve the robustness of deep models against uncertainty and ambiguity. We leverage neighborhood information in the valence-arousal space to adaptively construct emotiona distributions for training samples. We also consider the uncertainty of provided labels when incorporating them into the label distributions. Our method can be easily integrated into a deep network to obtain more training supervision and improve recognition accuracy. Intensive experiments on several datasets under various noisy and ambiguous settings show that our method achieves competitive results and outperforms recent state-of-the-art approaches.*

---
last_modified_on: "2021-04-22"
id: vqa-intro
title: A Brief Introduction to Visual Question Answering
author_github: https://github.com/Gazeal
tags: ["type: post", "AI", "Computer Vision", "VQA"]
---

## 1. Visual Question Answering - Overview

**Visual Question Answering (VQA)** aims to figure out a correct answer for a given question consistent with the visual content of a given image. The overarching goal of this issue is to create systems that can comprehend the contents of an image in the same way that humans do and communicate effectively about that image in natural language. It is indeed a challenging task as it necessitates the interaction and complementation of both image feature extractor and natural language processor.

There are two main variants of VQA which are Free-Form Opened-Ended (FFOE) VQA and Multiple Choice (MC) VQA. In FFOE VQA, an answer is a free-form response to a given image-question pair input, while in MC VQA, an answer is chosen from an answer list for a given image-question pair input. The discussion of VQA variants will be shared in the next post.

## 2. Approaches for solving VQA task.

There are three main approaches for VQA ([Sunny Katiyar et al. 2020](http://www.ijstr.org/final-print/jan2020/A-Survey-On-Visual-Questioning-Answering-Datasets-Approaches-And-Models.pdf)):

- ***Compositional VQA models**:* the questions are interpreted as a set of many sub-tasks.
- ***Bayesian and Question-Aware models:*** this method is not suitable for use in systems that respond to image-related questions. Since the algorithm based on this method does not try looking at the picture and instead predicts the response based on the Bayesian model by determining the probability of the words in the dataset's responses.
- ***Attention based models***: this method try to learn the interaction between image and question features in VQA task through a module called ***attention***. Then, the joint features got from that module are leveraged for answering the corresponding question.

The final one is the most successful approach since recent states of the arts included attention mechanisms.

## 3. Attention based VQA approach.

In general, attention based VQA approaches have **four main steps** (See Figure 1):

- **Visual Representation:** Encode the information from the image into vector(s) by using **Convolutional Neural Network (CNN)**
- **Textual Representation:** Encode the information of question into vector(s) by using **Embedding**.
- **Joint Representation:** A further step to learn the interaction between question(s) and image(s). Output joint features can be vector(s).
- **Answer prediction:** the joint features from the previous step are then passed through this module to obtain the predicted answer. This module is mostly formed by a **Classification**.

![Overall](https://vision.aioz.io/f/50f4694d718d47248440/?dl=1)


*<center>**Figure 1.** The general approach for Visual Question Answering.</center>*

### 3.1. **Visual Representation**

The basic attributes or aspects that clearly help us recognize a specific object, image, or something are known as features. The distinguishing characteristics are the distinct properties. When operating on a VQA dataset, we must extract the features of various images in order to separate the images based on specific features or aspects. Image features are one of the most important pieces of information for a VQA system to output the correct answer.

Convolutional neural networks have emerged as the gold standard for image pattern recognition. An input image is converted into image features after it is passed through a convolutional network. Each filter in a CNN layer detects various patterns, such as corners, vertex, shapes, curves, and symmetries (See Figure 2).

![https://images.deepai.org/glossary-terms/1db68a74ce8c4eb083d76199aef2e051/feature-extraction.jpg](https://images.deepai.org/glossary-terms/1db68a74ce8c4eb083d76199aef2e051/feature-extraction.jpg)

*<center>**Figure 2.** An example of feature extraction in classification task ([Source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455148/)).</center>*.

The majority of VQA literature employs CNNs for image processing. The network's final layer is removed, and the remaining network is used to extract image features. For image representation in VQA, objects in images represented by features extracted from an object detector such as the [Faster-RCNN bottom-up model](https://arxiv.org/pdf/1707.07998.pdf).

### 3.2. Textual **Representation**

Textual embeddings can be offered in a variety of ways. Count-based and frequency-based techniques such as count vectorization and TF-IDF are examples of older approaches. There are also prediction-based approaches such as a continuous bag of words and skip grams. Pretrained Word2Vec models are also openly accessible. Embeddings can also be created using deep learning architectures such as RNNs, LSTMs, GRUs, and 1-D CNNs. LSTMs are one of the most often used in VQA literature. For question embedding in VQA, [Glove](https://nlp.stanford.edu/pubs/glove.pdf) and [BERT](https://arxiv.org/pdf/1810.04805.pdf) are used widely for capturing the representation of words and sentences in different contexts (See Figure 3).

![https://www.researchgate.net/publication/344410741/figure/fig4/AS:940945646305280@1601350332222/Flowchart-of-generating-question-embedding-The-word-embedding-is-the-concatenation-of.jpg](https://www.researchgate.net/publication/344410741/figure/fig4/AS:940945646305280@1601350332222/Flowchart-of-generating-question-embedding-The-word-embedding-is-the-concatenation-of.jpg)

*<center>**Figure 3.** An example of question embedding for VQA ([Source](https://arxiv.org/pdf/2009.12770.pdf)).</center>*

### 3.3. Joint **Representation**

In current VQA systems, the joint modality component plays an essential role since it would learn meaningful joint representations between linguistic and visual inputs by applying the attention mechanism. There are many works that learn the interaction between question and image. However, the most solid and easy to understand is Bilinear Attention Network - BAN ([Kim et al. 2018](https://arxiv.org/pdf/1805.07932.pdf)). See Figure 4 for more details.

![https://github.com/jnhwkim/ban-vqa/blob/master/misc/ban_overview.png?raw=true](https://github.com/jnhwkim/ban-vqa/blob/master/misc/ban_overview.png?raw=true)

*<center>**Figure 4.** Bilinear Attention mechanism for VQA ([Source](https://arxiv.org/pdf/1805.07932.pdf)).</center>*

### 3.4. Answer Prediction

In most recent works, the joint features got from the attention mechanism is then passed through a classifier to output predicted answer. However, more modules can also be applied to produce external knowledge and deal with difficult questions. For instance, [Q.Li et al. 2018](https://arxiv.org/pdf/1801.09041.pdf) propose the Explainable Visual Question Answering by using Attributes and Captions as external information to deal with high semantic reasoning questions (See Figure 5).
![fig31](https://vision.aioz.io/f/8872a6622e1e400eba35/?dl=1)

*<center>**Figure 5.** Explainable Visual Question Answering ([Source](https://arxiv.org/pdf/1801.09041.pdf)).</center>*

---
last_modified_on: "2021-05-05"
id:  vqa-cti    
title: Compact Trilinear Interaction for Visual Question Answering    
description: "Visual Question Answering."    
author_github: https://github.com/Gazeal    
tags: ["type: post", "AI", "Computer Vision", "VQA", "Deep Learning"]
---

In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear interaction between the three inputs. Moreover, knowledge distillation is applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interactionmodel to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results on all three datasets.    
    
For free-form opened-ended VQA task, CTI achieved **67.4** on [VQA-2.0 dataset](https://visualqa.org/download.html) and **87.0** on [TDIUC dataset](https://kushalkafle.com/projects/tdiuc) in VQA accuracy metric.     
    
For multiple choice VQA task, CTI achieved **72.3** on [Visual7W dataset](https://github.com/yukezhu/visual7w-toolkit) in MC-VQA accuracy metric proposed in [here](http://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Visual7W_Grounded_Question_CVPR_2016_paper.pdf).     
    
## Compact Trilinear Interaction in VQA. 
Let $M = \{M_1, M_2, M_3\}$  be the representations of three inputs. $M_t \in \textbf{R}^{n_t \times d_t}$, where $n_t$ is the number of channels of the input $M_t$ and $d_t$ is the dimension of each channel.     
For example, if $M_1$ is the region-based representation for an image, then $n_1$ is the number of regions and $d_1$ is the dimension of the feature representation for each region. Let $m_{t_e} \in \textbf{R}^{1 \times d_{t}}$ be the $e^{th}$ row of $M_t$, i.e., the feature representation of $e^{th}$ channel in $M_t$, where $t \in \{1, 2, 3\}$.    
    
The input for training VQA is  set of $(V,Q,A)$ in which $V$ is an image representation; $V \in \textbf{R}^{v \times d_v}$ where $v$ is the number of interested regions (or bounding boxes) in the image and $d_v$ is the dimension of the representation for a region; $Q$ is a question representation; $Q \in \textbf{R}^{q \times d_q }$     
where $q$ is the number of hidden states and $d_q$ is the dimension for each hidden state.     
$A$ is an answer representation; $A \in \textbf{R}^{a \times d_a}$     
where $a$ is the number of hidden states and $d_a$ is the dimension for each hidden state.     
    
We firstly compute the  attention map $\mathcal{M}$ as follows:    
    
$$
\mathcal{M} = \sum^R_{r=1} {\llbracket  \mathcal{G}_r; V W_{v_r},  Q W_{q_r}, A W_{a_r}\rrbracket}
$$    
    
Then the joint representation $z$ is computed as follows:    
    
$$
z^T= \sum_{i=1}^{v}\sum_{j=1}^{q}\sum_{k=1}^{a} \mathcal{M}_{ijk}\left( V_{i}W_{z_v} \circ Q_{j}W_{z_q} \circ A_{k}W_{z_a}\right)
$$    
    
where $W_{v_r},W_{q_r}, W_{a_r}$  and $W_{z_v},W_{z_q}, W_{z_a}$ are learnable factor matrices; each $\mathcal{G}_r$ is a learnable Tucker tensor.    
    
    
## Integrate CTI into different VQA task 
### For multiple choice VQA    
 ![Fig1](https://vision.aioz.io/f/2bdd274ef72c4170a7ac/?dl=1)    
*<center>**Figure 1.** The model when CTI is applied to MC VQA.</center>*    
    
 Each input question and each answer are trimmed to a maximum of 12 words which will then be zero-padded if shorter than 12 words. Each word is then represented by a 300-D GloVe word embedding. Each image is represented by a $14 \times 14 \times 2048$ grid feature (i.e., $196$ cells; each cell is with a $2048$-D feature), extracted from the second last layer of ResNet-152 which is pre-trained on ImageNet.    
    
Input samples are divided into positive samples and negative samples. A positive sample, which is labelled as $1$ in binary classification, contains image, question and the right answer. A negative sample, which is labelled as $0$ in binary classification, contains image, question, and the wrong answer.  These samples are then passed through CTI to get the joint representation $z$. The joint representation is passed through a  binary classifier to get the prediction. The Binary Cross Entropy loss is used for training the model.    
    
### For free-form opened-ended VQA    
 ![Fig2](https://vision.aioz.io/f/41a54c1b2abf4b54821e/?dl=1)    
*<center>**Figure 1.** The model when CTI is applied to MC VQA.</center>*    
 Unlike MC VQA, FFOE VQA treats the answering as a classification problem over the set of predefined answers. Hence the set possible answers for each question-image pair is much more than the case of MC VQA. For each question-image input, the model takes every possible answers from its answer list to computed the joint representation, causes high computational cost.    
    
In addition, CTI requires all three $V, Q, A$ inputs to compute the joint representation. However, during the testing, there are no available answer information in FFOE VQA. To overcome these challenges, we propose to use Knowledge Distillation to transfer the learned knowledge from a teacher model to a  student model.     
    
The loss function for the student model is defined as:    
    
$$
\mathcal{L}_{KD} = \alpha T^2 \mathcal{L}_{CE}(Q^\tau_S, Q^\tau_T) + (1-\alpha)\mathcal{L}_{CE}(Q_S,y_{true})
$$    
    
where $\mathcal{L}_{CE}$ stands for Cross Entropy loss; $Q_S$ is the standard softmax output of the student; $y_{true}$ is the ground-truth answer labels;    
$\alpha$ is a  hyper-parameter for controlling the importance of each loss component; $Q^\tau_S, Q^\tau_T$ are the softened outputs  of the student and the teacher using the same temperature parameter $T$, which are computed as follows:    
    
$$
Q^\tau_i = \frac{exp(l_i/T)}{\sum_{i} exp(l_i/T)}
$$    
    
where for both teacher and the student models, the logit $l$ is the predictions outputted by the corresponding classifiers.     
    
## Results 
To further evaluate the effectiveness of CTI, we conduct a detailed comparison with the current state of the art. For FFOE VQA, we compare CTI with the recent state-of-the-art methods on TDIUC and VQA-2.0 datasets, including [SAN](https://arxiv.org/pdf/1511.02274.pdf), [QTA](https://arxiv.org/pdf/1804.02088.pdf), [MCB](https://arxiv.org/pdf/1606.01847.pdf), [BAN2](https://arxiv.org/pdf/1805.07932.pdf), and [RAU](https://arxiv.org/pdf/1606.03647.pdf). For MC VQA, we compare with the state-of-the-art methods on Visual7W dataset, including [BAN2](https://arxiv.org/pdf/1805.07932.pdf), [SAN](https://arxiv.org/pdf/1511.02274.pdf), [MLP](https://arxiv.org/pdf/1606.08390.pdf), [MCB](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Recurrent_Multimodal_Interaction_ICCV_2017_paper.pdf), [STL](https://arxiv.org/pdf/1801.07853v1.pdf), and [fPMC](https://arxiv.org/pdf/2005.01239.pdf).  
    
  <p align="center">    
    <img src="https://vision.aioz.io/f/2c9ad7aa988e49c599b0/?dl=1" alt>    
</p>    
<p align="center">    
    <em>    
    <b>Table 1.</b>Performance of CTI and BAN2, SAN in VQA-2.0 validation set and test-dev set. BAN2-CTI and SANCTI are student models trained under the teacher model. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.pdf">(Source)</a>.   
</em>    
</p>    
    
<p align="center">    
    <img src="https://vision.aioz.io/f/ec3f97eadcfa437a9de4/?dl=1" alt>    
</p>    
<p align="center">    
    <em>    
    <b>Table 2.</b> Performance  comparison  between  different  approaches with different evaluation metrics on TDIUC validation set. BAN2-CTI and SAN-CTI are the student models trained under compact trilinear interaction teacher model <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.pdf">(Source)</a>.   
</em>    
</p>    
    
Regarding FFOE VQA, Tables 1 and  2 show comparative results on VQA-2.0 and TDIUC respectively. Specifcaly, Table 1 shows that distilled student BAN2-CTI outperforms all compared methods over all metrics by a large margin, i.e., the model outperforms the current  state-of-the-art QTA on TDIUC by $3.4\%$ and $5.4\%$ on Ari and Har metrics, respectively.   The results confirm that trilinear interaction has learned informative representations from the three inputs and the learned information is effectively transferred to student models by distillation.    
    
<p align="center">    
    <img src="https://vision.aioz.io/f/dc6885e8453a4914af3b/?dl=1" alt>    
</p>    
<p align="center">    
    <em>    
    <b>Table 3.</b> Performance comparison between different approaches on Visual7W test set. Both training set and validation set are used for training. All models but CTIwBoxes are trained with same image and question representations. Both train set and validation set are used for training. Note that CTIwBoxes is the CTI model using <a href="https://arxiv.org/pdf/1707.07998.pdf">Bottom-up features</a>.  instead of grid features for image representation. <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.pdf">(Source)</a>.    
</em>    
</p>     
  
Regarding MC VQA, Table 3 shows that the CTI outperforms compared methods by a noticeable margin. This model outperforms the current state-of-the-art STL by 1.1%. Again, this validates the effectiveness of the proposed joint presentation learning, which precisely and simultaneously learns interactions between the three inputs. We note that when comparing with other methods on Visual7W, for image representations, we used the grid features extracted from ResNet-512 for a fair comparison. Our proposed model can achieve further improvements by using the object detection-based features used in FFOE VQA. With new features, the model denoted as CTIwBoxes in Table 3 achieve 72.3% accuracy with Acc-MC metric which improves over the current state-of-the-art STL 4.1%.  
  
## Conclusion 
A novel compact trilinear interaction  is introduced to simultaneously learns high level associations between image, question, and answer in both MC VQA and FFOE VQA. In addition, knowledge distillation is the first time applied to FFOE VQA to overcome the computational complexity and memory issue of the interaction. The extensive experimental results show that these models achieve the state-of-the-art results on three benchmarking datasets.      
    
## Open Source 
Github: https://github.com/aioz-ai/ICCV19_VQA-CTI
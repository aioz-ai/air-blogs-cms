---
last_modified_on: "2021-05-03"
id: vqa-mevf
title: Overcoming Data Limitation in Medical Visual Question Answering
description: A novel medical VQA framework that overcomes the labeled data limitation
author_github: https://github.com/xuanbinh-nguyen96
tags: ["type: post", "AI", "Computer Vision", "VQA", "Auto-decoder", "Meta-learning", "Medical"]
---

## What are the difficulties when dealing with Medical VQA task?

Visual Question Answering (VQA) aims to provide a correct answer to a given question such that the answer is consistent with the visual content of a given image.

In medical domain, VQA could benefit both doctors and patients. For example, doctors could use answers provided by VQA system as support materials in decision making, while patients could ask VQA questions related to their medical images for better understanding their health.

![Fig-1](https://vision.aioz.io/thumbnail/9b116d2c03f44db89b79/1024/vqa-mevf-Fig_1.png)  
*<center>***Figure 1**: An example of Medical VQA ([Source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6244189/pdf/sdata2018251.pdf)).*</center>*

However, one major problem with medical VQA is the lack of large scale labeled training data which usually requires huge efforts to build.

- The first attempt for building the dataset for medical VQA is by [ImageCLEF-Med](http://www.sadidhasan.com/sadid-VQAMed2018.pdf) [3]. In particular, in [3], images were automatically captured from PubMed Central articles. The questions and answers were automatically generated from corresponding captions of images. By that construction, the data has high noisy level, i.e., the dataset includes many images that are not useful for direct patient care and it also contains questions that do not make any sense.

- Recently, in [Lau et. al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6244189/pdf/sdata2018251.pdf) [1], the authors released the first manually constructed dataset VQA-RAD for medical VQA. Unfortunately, it contains only 315 images, which prevents to directly apply the powerful deep learning models for the VQA problem. One may think about the use of transfer learning  in which the pretrained deep learning models that are trained on the large scale labeled dataset such as ImageNet are used for finetuning on the medical VQA. However, due to difference in visual concepts between ImageNet images and medical images, finetuning with very few medical images is not sufficient.

Therefore it is necessary to develop a new VQA framework that can improve the accuracy while still only needs a small labeled training data.

The motivation for our approach to overcome the data limitation of medical VQA comes from two observations:

- Firstly, we observe that there are large scale unlabeled medical images available. These images are from same domain with medical VQA images. Hence if we train an unsupervised deep learning model using these unlabeled images, the trained weights may be easier to be adapted to the medical VQA problem than the pretrained weights on ImageNet images.

- Another observation is that although the labeled dataset VQA-RAD is primarily designed for VQA, by spending a little effort, we can extract the new class labels for that dataset. The new class labels allow us to apply the recent meta-learning technique for learning meta-weights, that can be quickly adapted to the VQA problem later.

## Methodology

The proposed medical VQA framework is presented in Figure 2. In our framework, the image feature extraction component is initialized by pretrained weights from MAML and CDAE. After that, the VQA framework will be finetuned in an end-to-end manner on the medical VQA data. In the following sections, we detail the architectures of MAML, CDAE, and our framework.

![Fig-2](https://vision.aioz.io/thumbnail/2f9e5e59b5ba458ebcd0/1024/vqa-mevf-Fig_2.png)  
*<center>***Figure 2**: The proposed medical VQA. The  image feature extraction is denoted as 'Mixture of Enhanced Visual Features (MEVF)' and is marked with the red dashed box. The weights of MEVF are intialized by MAML and CDAE.*</center>*

### Model-Agnostic Meta-Learning -- MAML

We follow [4] to design MAML. It consists of four $3\times3$ convolutional layers with stride $2$ and is ended with a mean pooling layer; each convolutional layer has $64$ filters and is followed by a ReLu layer.

We create the  dataset for training MAML by manually reviewing around three thousand question-answer pairs from the training set of VQA-RAD dataset. In our annotation process, images are split into three parts based on its *body part* labels (*head, chest, abdomen*). Images from each body part are further divided into three subcategories based on the interpretation from the question-answer pairs corresponding to the images. These subcategories are:
 1. *normal* images in which no pathology is found.
 2. *abnormal present* images in which there are the existence of fluid, air, mass, or tumor.
 3. *abnormal organ* images in which the organs are large in size or in wrong position.

Thus, all the images are categorized into 9 classes:
```
 | head normal      | head abnormal present     | head abnormal organ           |
 | chest normal     | chest abnormal organ      | chest abnormal present        |
 | abdominal normal | abdominal abnormal organ	| abdominal abnormal present    |
```
For every iteration of MAML training (line 3 in Alg.1),  5 tasks are sampled per iteration. For each task, we randomly select 3 classes (from 9 classes). For each class, we randomly select 6 images in which 3 images are used for updating task models and the remaining 3 images are used for updating meta-model.

![Alg-1](https://vision.aioz.io/thumbnail/ad998d779b1f4454a8be/1024/vqa-mevf-Alg_1.png)

### Denoising Auto Encoder -- CDAE

The encoder maps an image $x'$, which is the noisy version of the original image $x$, to a latent representation $z$ which retains  useful amount of information. The decoder transforms $z$ to the output $y$. The training algorithm aims to minimize the reconstruction error between $y$ and the original image $x$ as follows
$$L_{rec} = \left \| x-y \right \|_2^2$$

In our design, the encoder is a stack of convolutional layers; each of them is followed by a max pooling layer. The decoder is a stack of  deconvolutional and convolutional layers. The noisy version $x'$ is achieved by adding Gaussian noise to the original image $x$.

To train CDAE, we collect $11,779$ unlabeled images available online which are brain MRI images [2], [chest X-ray images](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) and [CT abdominal images](https://www.synapse.org/\#!Synapse:syn3193805/wiki/217753). The dataset is split into train set with $9,423$ images and test set with $2,356$ images. We use Gaussian noise to corrupt the input images before feeding them to the encoder.

### Our VQA framework

After training MAML and CDAE, we use their trained weights to initialize the MEVF image feature extraction component in the VQA framework. We then finetune the whole VQA model using the training set of VQA-RAD dataset.

To train the proposed model, we introduce a multi-task loss func-tion to incorporate the effectiveness of the CDAE to VQA. Formally, our lossfunction is defined as follows:
$$L = \alpha_1 L_{vqa} + \alpha_2 L_{rec}$$
where $L_{vqa}$ is a Cross Entropy loss for VQA classification and $L_{rec}$ stands for the reconstruction loss of CDAE . The whole VQA model is finetuned in an end-to-end manner.
## Results

![Tab-1](https://vision.aioz.io/thumbnail/6028e52eb90d4ed7a990/1024/vqa-mevf-Tab_1.png =500x)
*<center>***Table 1**: VQA results on VQA-RAD test set. All reference methods differ at the image feature extraction component. Other components are similar. The Stacked Attention Network (SAN) [6] is used as the attention mechanism in all methods.*</center>*

Table 1 presents VQA accuracy in both VQA-RAD open-ended and close-ended questions on the test set. The results  show that for both MAML and CDAE, by firstly pretraining then finetuning, the finetuning significantly improves the performance over the training from scratch using only VQA-RAD.

In addition, the results also show that our pretraining and finetuning of MAML and CDAE give better performance than the finetuning of VGG-16 which is pretrained on the  ImageNet dataset. Our proposed image feature extraction MEVF which leverages both pretrained weights of MAML and CDAE, then finetuning them give the best performance. This confirms the effectiveness of the proposed MEVF for  dealing with the limitation of labeled training data for medical VQA.


![Tab-2](https://vision.aioz.io/thumbnail/d15189a487024c5d8ed0/1024/vqa-mevf-Tab_2.png =700x)  
*<center>***Table 2**: Performance comparison on VQA-RAD test set.*</center>*

Table 2 presents comparative results between methods. Note that for the image feature extraction, the baselines use the pretrained models (VGG or ResNet) that have been trained on ImageNet and then finetune on the VQA-RAD dataset. For the question feature extraction, all baselines and our framework use the same pretrained models (i.e., Glove) and finetuning on VQA-RAD. The results show that when BAN or SAN is used as the attention mechanism in our framework, it significantly outperforms the baseline frameworks  BAN [5] and SAN [6]. Our best setting, i.e. the one with BAN as the attention, achieves the state-of-the-art results and it significantly outperforms the best baseline framework  BAN [5], i.e., the improvements are $16.3\%$ and $8.6\%$ on open-ended and close-ended VQA, respectively.

## Conclusion

In this paper, we proposed a novel medical VQA framework that leverages the meta-learning MAML and denoising auto-encoder CDAE for image feature extraction in order to overcome the limitation of labeled training data.  Specifically, CDAE helps to leverage information from the large scale unlabeled images, while MAML helps to learn meta-weights that can be quickly adapted to the VQA problem. We establish new state-of-the-art results on VQA-RAD dataset for both close-ended and open-ended questions.

## Open Source
:octocat: Github: https://github.com/aioz-ai/MICCAI19-MedVQA

### Reference
[1] Lau, Jason J., et al. "A dataset of clinically generated visual questions and answers about radiology images." Scientific data, 2018.

[2] Clark, Kenneth, et al. "The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository." Journal of digital imaging, 2013.

[3] Hasan, Sadid A., et al. "Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task." CEUR Work-shop Proceedings, 2018.

[4] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." In ICML, 2017.

[5] Kim, Jin-Hwa, Jaehyun Jun, and Byoung-Tak Zhang. "Bilinear attention networks." In NIPS, 2018.

[6] Yang, Zichao, et al. "Stacked attention networks for image question answering." In CVPR, 2016.
